{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdal\n",
    "import nivapy3 as nivapy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for image classification\n",
    "\n",
    "Kasper has supplied some example images for the ImageDrone DigiSIS project - see e-mails received 07/03/2018 for details.\n",
    "\n",
    "The aim of this notebook is to train a simple machine learning algorithm to automatically classify marine substrates. The main categories of interest are:\n",
    "\n",
    " * Sandy sediments\n",
    " * Eel grass\n",
    " * Brown seaweeds\n",
    " \n",
    "We might also include \"Fucus\" here, although I've ignored this for now (see e-mail from Kapser received 07/03/2018 at 23:44).\n",
    " \n",
    "In addition, based on a brief look at the images, I have defined the following additional categories:\n",
    "\n",
    " * Rock (terrestrial)\n",
    " * Beach (terrestrial)\n",
    " * Vegetation (terrestrial)\n",
    " \n",
    "This second group is not of scientific interest, but it may nevertheless be useful to distinguish them explicitly.\n",
    "\n",
    "## 1. Overview of raw data\n",
    "\n",
    "The raw images are \"geotagged\" (but not georeferenced) JPGs. In other words, the metadata in the \"Properties\" for each image provides the latitude, longitude and altitude, but the image extent, pixel dimensions, warp etc. cannot be determined without further processing. Creating fully georeferenced images requires combining multiple JPGs and applying an orthographic correction. This can be done using e.g. [OpenDroneMap](http://opendronemap.org/) (see discussion with Kapser and Robert on 07/03/2018 for details), but for now I will focus on working with the \"raw\", unreferenced datasets. \n",
    "\n",
    "In practice, working with the raw data means defining a \"false projection\" for each image (WGS84 UTM Zone 32N in this case), but not actually performing any georeferencing i.e. each image appears at the \"origin\" of the UTM Zone 32N grid. In addition, it is more convenient to work with GeoTiffs than JPGs. The folder `'/gis/raster/geotiff_unproj'` therefore contains `.tif` versions of the raw JPGs, with the \"false\" co-ordinate system applied.\n",
    "\n",
    "## 2. Manual classification\n",
    "\n",
    "I need to create a reference dataset for training and evaluation of the ML algorithm. This is achieved by manually classifying a selection of images, and then using some of them for training and some for evaluation. In some cases, there will also be uncertainty regarding the manual classifications, but this is ignored in the analysis presented here: I'm assuming the manual classifications represent the \"truth\", and the ML algorithm is evaluated based on its ability to reproduce the manually assigned categories.  \n",
    "\n",
    "Constructing a good classification algorithm requires a good training dataset, but for the preliminary exploration presented here I'm **going to focus on just three of the images** supplied by Kapser:\n",
    "\n",
    " * DJI_0388 \n",
    " * DJI_0389 \n",
    " * DJI_0399\n",
    " \n",
    "The raw image named `'templateDJI_0388_characterized.jpg'` (provided by Kasper and shown below) gives examples of the main substrate categories under consideration. I have used this image to guide a more comprehensive classification of the three images listed above.\n",
    "\n",
    "**Note:** I believe there is also a \"ground truth\" dataset, collected by manually checking the substrate at specific points from a boat. I do not yet have access to this data and so have not included it here, but it would probably help to refine/improve the training dataset.\n",
    "\n",
    "<img src=\"../images/raw_drone_jpgs/templateDJI_0388_characterized.jpg\" alt=\"Substrate examples\" width=\"800\" />\n",
    "\n",
    "### 2.1. Manual classification workflow\n",
    "\n",
    "Manual classification was performed using ArcGIS. I've created a geodatabase called `'drone_training_data.gdb'`, which includes a Coded Domain called `'substrate'` with the following classes:\n",
    "\n",
    "| Code |      Substrate Class     |\n",
    "|:----:|:------------------------:|\n",
    "|  -1  |           Other          |\n",
    "|   0  |          No data         |\n",
    "|   1  |         Eel grass        |\n",
    "|   2  |      Sandy sediment      |\n",
    "|   3  |      Brown seaweeds      |\n",
    "|   4  |           Fucus          |\n",
    "|   5  |    Rock (terrestrial)    |\n",
    "|   6  |    Beach (terrestrial)   |\n",
    "|   7  | Vegetation (terrestrial) |\n",
    "\n",
    "I have then created a feature dataset for the \"unprojected\" vector data (with the same \"false\" projection used by the GeoTiffs), and within this there is one feature class per image, each with the coded domain shown above assigned to the `'substrate'` field.\n",
    "\n",
    "This setup makes it relatively easy to create a training dataset in ArcGIS, as illustrated by the screenshot below.\n",
    "\n",
    "<img src=\"../images/arcgis_screenshot.png\" alt=\"ArcGIS screenshot\" width=\"1000\" />\n",
    "\n",
    "### 2.2. Rasterise vector training data\n",
    "\n",
    "The vector feature classes must be rasterised with the same extent and pixel resolution as the underlying GeoTiffs. This is most easily achieved by exporting from the geodatabase as shapefiles and then using NivaPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class codes\n",
    "class_codes = {-1:'Other',\n",
    "               0: 'No data',\n",
    "               1: 'Eel grass',\n",
    "               2: 'Sandy sediment',\n",
    "               3: 'Brown seaweeds', \n",
    "               4: 'Fucus',\n",
    "               5: 'Rock (terrestrial)',\n",
    "               6: 'Beach (terrestrial)',\n",
    "               7: 'Vegetation (terrestrial)'}\n",
    "\n",
    "# Use one of the raw images as a \"snap raster\"\n",
    "snap_tif = r'../gis/raster/geotif_unproj/DJI_0388.tif'\n",
    "\n",
    "# Image names\n",
    "fnames = ['DJI_0388', 'DJI_0389', 'DJI_0399']\n",
    "\n",
    "# Loop over shapefiles\n",
    "for name in fnames:\n",
    "    # Build path to shp\n",
    "    shp_path = r'../gis/vector/%s.shp' % name\n",
    "    \n",
    "    # Build path to output tiff\n",
    "    tif_path = r'../gis/raster/geotif_unproj/%s_manual.tif' % name\n",
    "    \n",
    "    # Rasterise\n",
    "    nivapy.spatial.shp_to_ras(shp_path, tif_path, snap_tif, 'substrate', \n",
    "                              -1, gdal.GDT_Int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualise the raw images and the training data side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0ae8de06884428ad5114faf7e6182c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup plot\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10,10))\n",
    "\n",
    "# Define colours for manual classes\n",
    "cmap = colors.ListedColormap(['white', 'black', 'mediumseagreen', 'gold',\n",
    "                              'tan', 'sandybrown', 'lightgrey', 'orange', \n",
    "                              'lawngreen'])\n",
    "bounds = np.arange(-1.5, 8.5)\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# Loop over data\n",
    "for idx, name in enumerate(fnames):\n",
    "    # Paths to images\n",
    "    raw_path = r'../gis/raster/geotif_unproj/%s.tif' % name\n",
    "    man_path = r'../gis/raster/geotif_unproj/%s_manual.tif' % name\n",
    "    \n",
    "    # Read raw bands to arrays\n",
    "    band1, ndv, extent = nivapy.spatial.read_geotiff(raw_path, band=1)\n",
    "    band2, ndv, extent = nivapy.spatial.read_geotiff(raw_path, band=2)\n",
    "    band3, ndv, extent = nivapy.spatial.read_geotiff(raw_path, band=3)\n",
    "    raw_img = np.dstack((band1, band2, band3))\n",
    "    \n",
    "    # Read manually classified data (1 band only)\n",
    "    man_img, ndv, extent = nivapy.spatial.read_geotiff(man_path, band=1)\n",
    "    \n",
    "    # Plot\n",
    "    # Raw image\n",
    "    axes[idx, 0].imshow(raw_img)\n",
    "    axes[idx, 0].set_title('%s (raw image)' % name, fontsize=12)\n",
    "    \n",
    "    # Manually classified\n",
    "    img = axes[idx, 1].imshow(man_img, cmap=cmap, norm=norm)\n",
    "    axes[idx, 1].set_title('%s (classified)' % name, fontsize=12)\n",
    "    \n",
    "    # Turn off axes\n",
    "    axes[idx, 0].axis('off')\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    # Colourbar for manual dataset\n",
    "    cb = plt.colorbar(img, ax=axes[idx, 1])    \n",
    "    labels = np.arange(-1, 8)\n",
    "    cb.set_ticks(labels)\n",
    "    cb.set_ticklabels(labels)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and evaluation datasets\n",
    "\n",
    "I will take a standard ML approach and split the data into three portions: one for training, one for evaluation and a third for testing. This preliminary analysis will focus on pixel-based approaches, and each image is 4608 by 3456 pixels in size. We therefore have a total of $4608 \\times 3456 \\times 3 = 47.8 \\; megapixels$ to work with. Each image consists of three bands (R, G, B), each of which will form an \"explanatory variable\". In ML terms, we therefore have a design matrix consisting of 47.8 million samples and three features. I will split this data as follows:\n",
    "\n",
    " * Data from the first two images (`'DJI_0388'` and `'DJI_0389'`) will be pooled, and then split 60:40 at random to create the training and evaluation datasets, respectively. These datasets will have the following sizes:\n",
    " \n",
    "$$Training: \\qquad 0.6 \\times 4608 \\times 3456 \\times 2 = 19.1 \\; million \\; samples$$\n",
    "\n",
    "$$Evaluation: \\qquad 0.4 \\times 4608 \\times 3456 \\times 2 = 12.7 \\; million \\; samples$$\n",
    "\n",
    "  * The thrid image, `'DJI_0399'`, is withheld as a completely independent image for testing the fitted algorithm at the end of the process (see section 5, below).\n",
    "  \n",
    "**Note:** These three images are not entirely independent - there is substantial overlap between them. In fact, image 3 is approximately the same as image 2, just rotated by 180 degrees (see above). The testing procedure would be more robust if it applied to entirely separate images (e.g. from a different area and/or under different lightling conditions). This needs exploring eventually, but is not considered here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:   19106150\n",
      "Number of evaluation samples: 12737434\n"
     ]
    }
   ],
   "source": [
    "# Containers for data\n",
    "y_list = []\n",
    "r_list = []\n",
    "g_list = []\n",
    "b_list = []\n",
    "\n",
    "# Loop over data for images 1 and 2\n",
    "for name in fnames[:2]:\n",
    "    # Paths to images\n",
    "    raw_path = r'../gis/raster/geotif_unproj/%s.tif' % name\n",
    "    man_path = r'../gis/raster/geotif_unproj/%s_manual.tif' % name\n",
    "    \n",
    "    # Read raw bands to arrays\n",
    "    band1, ndv, extent = nivapy.spatial.read_geotiff(raw_path, band=1)\n",
    "    band2, ndv, extent = nivapy.spatial.read_geotiff(raw_path, band=2)\n",
    "    band3, ndv, extent = nivapy.spatial.read_geotiff(raw_path, band=3)\n",
    "    \n",
    "    # Read manually classified data (1 band only)\n",
    "    man_img, ndv, extent = nivapy.spatial.read_geotiff(man_path, band=1)\n",
    "    \n",
    "    # Flatten to 1D and append\n",
    "    y_list.append(man_img.flatten())\n",
    "    r_list.append(band1.flatten())\n",
    "    g_list.append(band2.flatten())\n",
    "    b_list.append(band3.flatten())\n",
    "    \n",
    "# Concatenate to single dataset\n",
    "y = np.concatenate(y_list)\n",
    "r = np.concatenate(r_list)\n",
    "g = np.concatenate(g_list)\n",
    "b = np.concatenate(b_list)\n",
    "\n",
    "# Build df\n",
    "df = pd.DataFrame({'y':y,\n",
    "                   'r':r,\n",
    "                   'g':g,\n",
    "                   'b':b})\n",
    "\n",
    "# Some values are \"No data\" due to edge effects. Remove these\n",
    "df = df.query('y > 0')\n",
    "\n",
    "# Split into training and evaluation\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df[['r', 'g', 'b']],\n",
    "                                                    df['y'],\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Checking\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_eval) == len(y_eval)\n",
    "print('Number of training samples:  ', len(X_train))\n",
    "print('Number of evaluation samples:', len(X_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and evaluate classifier\n",
    "\n",
    "We will begin with a Random Forest Classifier, which is relatively easy to implement and has some nice properties (such as providing class probabilities for each pixel).\n",
    "\n",
    "### 4.1. Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 14s, sys: 13.2 s, total: 15min 27s\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fit classifier to training data\n",
    "classifier = RandomForestClassifier(n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 4.71 s, total: 1min 6s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Predict classes for remaining 40% of data\n",
    "preds = classifier.predict(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more than 19 million samples, it takes quite a long time (~4.5 minutes) to train the classifier. However, once trained, making predictions is relatively fast (~20 seconds).\n",
    "\n",
    "### 4.2. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "               Eel grass       0.87      0.86      0.86   4398868\n",
      "          Sandy sediment       0.78      0.79      0.79   2931543\n",
      "          Brown seaweeds       0.87      0.87      0.87   4634310\n",
      "      Rock (terrestrial)       0.81      0.80      0.80    673075\n",
      "     Beach (terrestrial)       0.71      0.61      0.66     58747\n",
      "Vegetation (terrestrial)       0.79      0.40      0.53     40891\n",
      "\n",
      "             avg / total       0.84      0.84      0.84  12737434\n",
      "\n",
      "Classification accuracy: 0.842857\n"
     ]
    }
   ],
   "source": [
    "# Only use relevant labels from the training dataset\n",
    "class_labels = [1, 2, 3, 5, 6, 7]\n",
    "target_names = [class_codes[i] for i in class_labels]\n",
    "\n",
    "# Print metrics\n",
    "print(\"Classification report:\\n%s\" %\n",
    "      metrics.classification_report(y_eval, preds, \n",
    "                                    labels=class_labels, \n",
    "                                    target_names=target_names))\n",
    "\n",
    "print(\"Classification accuracy: %f\" %\n",
    "      metrics.accuracy_score(y_eval, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the table above, the following definitions may be helpful (taken from [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)):\n",
    "\n",
    "Precision ($P$) is defined as the number of true positives ($T_p$) over the number of true positives plus the number of false positives ($F_p$)\n",
    "\n",
    "$$P = \\frac{T_p}{T_p+F_p}$$\n",
    "\n",
    "Intuitively, **precision** is the ability of the classifier **not to label as positive a sample that is negative**.\n",
    "\n",
    "Recall ($R$) is defined as the number of true positives ($T_p$) over the number of true positives plus the number of false negatives ($F_n$)\n",
    "\n",
    "$$R = \\frac{T_p}{T_p + F_n}$$\n",
    "\n",
    "Intuitively, **recall** is the ability of the classifier **to find all the positive samples**.\n",
    "\n",
    "These quantities are also related to the $F_1$ score, which is defined as the harmonic mean of precision and recall\n",
    "\n",
    "$$F1 = 2\\frac{P \\times R}{P+R}$$\n",
    "\n",
    "For each class, it gives an overall indication of performance, where precision and recall are weighted equally (see [here](https://en.wikipedia.org/wiki/F1_score) for more details).\n",
    "\n",
    "The overall classification accuracy is around 84% (i.e. 84% of the pixels in the evaluation dataset are classified correctly). From the $F_1$ scores, we see that \"Eel grass\" and \"Brown seaweeds\" are classified especially well - probably because they are the most common categories in the dataset (the `'support'` column in the table above gives the number of pixels for each class). \"Sandy sediment\" is less reliably classified, but still not bad. Terrestrial vegetation and beach areas are not well classified at all, but this is likely because the categories are under-represented in the training dataset.\n",
    "\n",
    "The **confusion matrix** provides further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',\n",
    "                          cmap='coolwarm'):\n",
    "    \"\"\" Plots the confusion matrix.\n",
    "    \n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \n",
    "        From:\n",
    "        \n",
    "        http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(shrink=0.6)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86568d727e8d42508fb80406e4f26995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Congusion matrix    \n",
    "fig = plt.figure(figsize=(8,8))\n",
    "fig.add_subplot(111)\n",
    "cm = metrics.confusion_matrix(y_eval, preds)\n",
    "plot_confusion_matrix(cm, classes=target_names, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values along the diagonal in a normalised confusion are the same as the \"Recall\" scores in the classification report above. The matrix should be read one row at a time: for example, the first row indicates that 86% of the true \"Eel grass\" pixels were correctly identified, but 9% were classified as \"Sandy sediment\" and 5% as \"Brown seaweeds\". In general, the algorithm is able to confidently identify the three main classes of interest, and the greatest confusion is associated with terrestrial vegetation, which is frequently mis-classified as \"Sandy sediment\".\n",
    "\n",
    "## 5. Predictions with a test image\n",
    "\n",
    "Next, the trained algorithm is presented with a new image (`'DJI_0399'`) and used to create a categorical map. This map can be compared to the manually classified version to get an impression of the algorithm's practical (rather than statistical) performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                   Other       0.00      0.00      0.00      3456\n",
      "               Eel grass       0.58      0.91      0.71   3158346\n",
      "          Sandy sediment       0.70      0.72      0.71   4137923\n",
      "          Brown seaweeds       0.73      0.70      0.71   4849191\n",
      "      Rock (terrestrial)       0.77      0.52      0.62   2931510\n",
      "     Beach (terrestrial)       0.12      0.00      0.00    701709\n",
      "Vegetation (terrestrial)       0.32      0.22      0.26    143113\n",
      "\n",
      "             avg / total       0.67      0.68      0.66  15925248\n",
      "\n",
      "Classification accuracy: 0.677661\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac9248ce3bc47b39e843277702af8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get image name\n",
    "name = fnames[-1]\n",
    "\n",
    "# Paths to images\n",
    "raw_path = r'../gis/raster/geotif_unproj/%s.tif' % name\n",
    "man_path = r'../gis/raster/geotif_unproj/%s_manual.tif' % name\n",
    "    \n",
    "# Read raw bands to arrays\n",
    "band1, ndv, extent = nivapy.spatial.read_geotiff(raw_path, band=1)\n",
    "band2, ndv, extent = nivapy.spatial.read_geotiff(raw_path, band=2)\n",
    "band3, ndv, extent = nivapy.spatial.read_geotiff(raw_path, band=3)\n",
    "\n",
    "# Read manually classified data (1 band only)\n",
    "man_img, ndv, extent = nivapy.spatial.read_geotiff(man_path, band=1)\n",
    "    \n",
    "# Build df\n",
    "df = pd.DataFrame({'r':band1.flatten(),\n",
    "                   'g':band2.flatten(),\n",
    "                   'b':band3.flatten()})\n",
    "\n",
    "# Predict classes for new image\n",
    "pred_img = classifier.predict(df[['r', 'g', 'b']]).reshape(man_img.shape)\n",
    "\n",
    "# Print metrics\n",
    "\n",
    "# The training dataset only includes some of the possible labels\n",
    "# Extract these for now\n",
    "class_labels = [-1, 1, 2, 3, 5, 6, 7]\n",
    "target_names = [class_codes[i] for i in class_labels]\n",
    "\n",
    "print(\"Classification report:\\n%s\" %\n",
    "      metrics.classification_report(man_img.flatten(), \n",
    "                                    pred_img.flatten(), \n",
    "                                    labels=class_labels, \n",
    "                                    target_names=target_names))\n",
    "\n",
    "print(\"Classification accuracy: %f\" %\n",
    "      metrics.accuracy_score(man_img.flatten(), pred_img.flatten()))\n",
    "\n",
    "# Congusion matrix    \n",
    "fig = plt.figure(figsize=(8,8))\n",
    "fig.add_subplot(111)\n",
    "cm = metrics.confusion_matrix(man_img.flatten(), pred_img.flatten())\n",
    "plot_confusion_matrix(cm, normalize=True, classes=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef98a245e62146d885b9f4297bab4708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot results\n",
    "# Setup plot\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8,12))\n",
    "   \n",
    "# Stack bands\n",
    "raw_img = np.dstack((band1, band2, band3))\n",
    "\n",
    "# Raw image\n",
    "axes[0].imshow(raw_img)\n",
    "axes[0].set_title('%s (raw image)' % name, fontsize=12)\n",
    "\n",
    "# Manually classified\n",
    "mimg = axes[1].imshow(man_img, cmap=cmap, norm=norm)\n",
    "axes[1].set_title('%s (manual)' % name, fontsize=12)\n",
    "\n",
    "# Predicted\n",
    "aimg = axes[2].imshow(pred_img, cmap=cmap, norm=norm)\n",
    "axes[2].set_title('%s (algorithm)' % name, fontsize=12)\n",
    "\n",
    "# Turn off axes\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Colourbar for manual dataset\n",
    "mcb = plt.colorbar(mimg, ax=axes[1])    \n",
    "labels = np.arange(-1, 8)\n",
    "mcb.set_ticks(labels)\n",
    "mcb.set_ticklabels(labels)\n",
    "\n",
    "# Colourbar for manual dataset\n",
    "acb = plt.colorbar(aimg, ax=axes[2])    \n",
    "labels = np.arange(-1, 8)\n",
    "acb.set_ticks(labels)\n",
    "acb.set_ticklabels(labels)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an initial attempt, I'd say these results are fairly promising. The automatically generated map is certainly similar to the manually classified version, though it's also obvious that some regionalisation/object segmentation would be beneficial.\n",
    "\n",
    "$F_1$ scores for the three main categories of interest are around 71% to 72%, which is reasonable as a starting point given the very limited training dataset. It is evident from the confusion matrix that the algorithm is not yet capable of confidently identifying sandy sediments and brown seaweeds, so incorporating more data on these categories would be a good idea.\n",
    "\n",
    "One issue that can easily be corrected is that ArcGIS has a minor bug, which leads to slight misalignments between vector and raster datasets. This results in a single row of \"No data\" cells along the margin of the predicted image, and these cells cannot be classified because the algorithm has never seen \"No data\" pixels before. This is the explanation for the \"Other\" column in the summary results above (with a `'support'` of 3456 - exactly one row of pixels). This has a negative impact on the predictive ability of the algorithm, and should be removed.\n",
    "\n",
    "## 6. Next steps\n",
    "\n",
    " * **Expand and improve the training dataset** (more varied light conditions, other categories of interest etc.). Also make use of the \"ground truth\" data from the boat\n",
    " \n",
    " * **Georeference, align and resample all images**, so that we can incorporate information from other spectral bands, not just (R, G, B)\n",
    " \n",
    " * **Try a range of classifiers/algorithms**\n",
    " \n",
    " * **Try image segmentation/OBIA**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
